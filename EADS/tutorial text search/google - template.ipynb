{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok, Google? #\n",
    "\n",
    "As soon as number of texts, objects, records and so on becomes bigger, users start suffering from waiting while performing search queries. Computer scientist invented different tricks to speed up search. Let's explore few such techniques.\n",
    "\n",
    "**Here's your model problem**. Your friend and you want to spend cool time playing [Jeopardy](https://en.wikipedia.org/wiki/Jeopardy!). One asks questions, other answers. You found a big database of questions. You don't what to play random questions, but only those related to some topics. So, your friend and you build a search engine that returns questions that contain **exact substring**<sup>1</sup>.\n",
    "\n",
    "<sup>1</sup> - generally search engines don't bother about **exact** matches, as users do a lot of typos, simplifications, ommit articles. But you and your friend want to experience difference between exact and approximate search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda holds most of requirements, but please make sure you have `requests` and `numpy` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sortedcontainers\n",
      "  Using cached https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
      "Installing collected packages: sortedcontainers\n",
      "Successfully installed sortedcontainers-2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#! pip install --user requests\n",
    "! pip install sortedcontainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All necessary requirements are imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, requests, copy\n",
    "import csv, re, random, sortedcontainers # I use SortedDict at the last phase. Maybe you will also do.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start preparing the data. Run next 3 blocks once to load everything to RAM (yes, we will fit in RAM for this tutorial).\n",
    "\n",
    "Firstly, load stop-words list. These are the words, that are not considered informative in index structures, as they are met in practically any text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: only don its did hers or when s these above they is ourselves during once so no same yours now him very again doing most those through yourselves and of am he we how themselves where herself can more while myself this does up an should over under nor both\n"
     ]
    }
   ],
   "source": [
    "stopwords_url = \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/\"\n",
    "stopwords = set(requests.get(stopwords_url).text.split())\n",
    "print(\"Sample:\", *list(stopwords)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load questions and answers into RAM. (Actually we will not use answers in this tutorial, but maybe you will play Jeopardy indeed?). `jeop.csv` file is attached to your tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216930 rows loaded in 3620.30 ms\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "texts = []\n",
    "words = []\n",
    "answers = {}\n",
    "with open(\"jeop.csv\", encoding=\"utf8\") as csvfile:\n",
    "    for line in csv.DictReader(csvfile, delimiter=',', quotechar='`'):\n",
    "        question = line[\"Question\"].lower()\n",
    "        answer = (line[\"Answer\"] or \"\").lower()\n",
    "        words = re.findall(\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", question)\n",
    "        texts.append(question)\n",
    "        answers[question] = answer\n",
    "        \n",
    "finish = time.time()\n",
    "print(\"{} rows loaded in {:.2f} ms\".format(len(texts), (finish - start) * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last block - test texts and queries. Just to keep test simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"Who let the dogs out?\",\n",
    "    \"Let's go, guys!\",\n",
    "    \"Mamma mia!!!\",\n",
    "    \"Here is the first sentence. And here is the second...\"\n",
    "]\n",
    "\n",
    "test_queries = [\n",
    "    \"go home\",\n",
    "    \"who said\",\n",
    "    \"what was the\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Naive substring search ##\n",
    "\n",
    "**Please write here the most naive substring search you can :)**\n",
    "\n",
    "- `query` - substring you are looking for.\n",
    "- `texts` - `list` of any other iterable collections of `string`s.\n",
    "- `returns` - list of string containing `query` as a substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"`i'll go home with bonnie jean` is one of many lively songs in this lerner & loewe musical\",\n",
       " \"you can `go home again` to see this author's boyhood home in asheville, north carolina\",\n",
       " \"in the '30s wolfe wrote `you can't go home again` & kaufman & hart wrote `you can't` do this\",\n",
       " 'he wrote, `i come to my solitary woodland walk as the homesick go home`:',\n",
       " \"<a href=`http://www.j-archive.com/media/2009-01-01_j_06.mp3`>`i won't think of it now... i'll go home to tara tomorrow`</a>\",\n",
       " \"he was born at home oct. 3, 1900 in asheville, n.c. & later found out you can't go home again\",\n",
       " '`better be worth it.  he better go home and cure some disease or invent a longer-lasting light bulb`',\n",
       " 'when nancy pelosi & maxine waters go home from the u.s. house, they go to this state',\n",
       " 'a traditional variation on `show me the way to go home` is `indicate the direction of my habitual` this',\n",
       " \"after the gauls besieged this city in 390 b.c., they were bought off so they'd go home\",\n",
       " \"win an academy award & you'll go home with this golden guy\",\n",
       " \"this semisonic song says, `you don't have to go home but you can't stay here`\",\n",
       " \"in `ballad of a thin man`, a one-eyed midget screams, `you're` one of these, `give me some milk or else go home`\",\n",
       " 'the `wine` is too warm in the transylvanian alps of this country, & why do i have to carry garlic?  me wanna go home',\n",
       " \"the summers are nice in this capital of manitoba--that's when i should have come. me wanna go home\",\n",
       " 'the seafood was great, the tequila too much in this `true cross` city on the gulf of mexico\";\" me wanna go home',\n",
       " 'if you can\\'t stand the heat either, then let\\'s caravan out of this fabled city at the center of mali\";\" me wanna go home',\n",
       " \"(audio daily double): mode of transportation mentioned in the following folk song: `not a shirt on my back, not a penny to my name, lord, i can't go home this way...`\",\n",
       " \"`almost like being in love`, `i'll go home with bonnie jean`\",\n",
       " \"`you're all clear, kid, now let's blow this thing & go home` is spoken just before this is destroyed in a 1977 movie\",\n",
       " 'in the `spring` of 1968, students in this capital were climbing on soviet tanks & yelling, `ussr go home!`',\n",
       " \"`daylight come and me wan' go home` is the last line of this song, also known as `day-o`\",\n",
       " \"a current supreme court justice finds `you can't go home again` as this author\",\n",
       " \"homer's roamer he was, for after a war, he tried to go home, for 9 years...no, more!\",\n",
       " 'after signing the surrender, he told his confederate troops to go home & resume their occupations',\n",
       " \"`you can't go home again` until you name this author who wrote it\",\n",
       " \"tho he `looked homeward` to north carolina, he lived in nyc because `you can't go home again`\",\n",
       " \"the home he described in `you can't go home again` is actually on 11th, not 12th st., as he wrote in the book\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_naive(query, texts):\n",
    "    result = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if text.find(query) != -1:\n",
    "            result.append(text)\n",
    "            \n",
    "    assert all([query in text for text in result])\n",
    "    return result\n",
    "search_naive(test_queries[0], texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: go home\n",
      "Time: 76.91 ms\n",
      "Size: 28\n",
      "Sample: [\"`i'll go home with bonnie jean` is one of many lively songs in this lerner & loewe musical\", \"you can `go home again` to see this author's boyhood home in asheville, north carolina\"]\n",
      "\n",
      "Query: who said\n",
      "Time: 61.53 ms\n",
      "Size: 54\n",
      "Sample: ['film character who said, `you should be kissed, and often, and by someone who knows how`', '`manhattan`ite who said, `life is divided into the horrible and the miserable`--sounds `bananas` to us']\n",
      "\n",
      "Query: what was the\n",
      "Time: 58.03 ms\n",
      "Size: 16\n",
      "Sample: ['on may 4, 1980 tito died in ljubljana in what was then this country', \"(<a href=`http://www.j-archive.com/media/2007-02-27_dj_29.jpg` target=`_blank`>jimmy of the clue crew reports from the pentagon.</a>)  i'm at the building put up between 1941 & 1943 to house what was then this government department\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in test_queries:\n",
    "    print(\"Query:\", query)\n",
    "    start = time.time()\n",
    "    ans = search_naive(query, texts)\n",
    "    finish = time.time()\n",
    "    print(\"Time: {:.2f} ms\".format(1000 * (finish - start)))\n",
    "    print(\"Size:\", len(ans))\n",
    "    print(\"Sample:\", ans[:2])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad... So fast? But this search is linear with respect to the number of text - `O(N)`. Thus, 200M texts will require 1000 times longer time. Shall you wait for 30 seconds each time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inverted index ##\n",
    "Inverted index is a beatiful concept, used in major search engines up to 2017. For details please refer to p. 234 of \"Algorithms in the real world\" lecture notes.\n",
    "### 1.1. Text to tokens ###\n",
    "Firstly, we need to extract the grains of the text. They should not be very small (letters will not give us any profit) and not very big (almost all sentences are unique). Words are good: there are few thousands of them, which is not very big number for hash table, but still big enough to catch the difference amoung texts.\n",
    "\n",
    "There are multiple approaches to extracting words from sentences. \"Correct\" approach is to use formal language model and run a parser (tokenizer) that will extract and label all the words and punktuation. \"Simpler\" approach (either split+strip, or regular expressions) have a benefit - they don't need any additional libraries like `nltk` :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_words(txt):\n",
    "    # Here we lower case and split the sentence into words\n",
    "    result = re.findall(\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", txt.lower()) # регулярное выражение\n",
    "    \n",
    "    assert bool(result) == bool(txt.strip()), \"Empty list for empty string only.\"\n",
    "    if bool(result):\n",
    "        assert all([w in txt.lower() for w in result]), \"Tokens are substrings of the text.\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for splitting. Assertions should not fail, splitting should look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who let the dogs out? => ['who', 'let', 'the', 'dogs', 'out']\n",
      "Let's go, guys! => [\"let's\", 'go', 'guys']\n",
      "Mamma mia!!! => ['mamma', 'mia']\n",
      "Here is the first sentence. And here is the second... => ['here', 'is', 'the', 'first', 'sentence', 'and', 'here', 'is', 'the', 'second']\n"
     ]
    }
   ],
   "source": [
    "for text in test_texts:\n",
    "    print(text, \"=>\", sentence_to_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Remove stopwords ###\n",
    "One important heuristic about indexing. There is relatively few words (say, few dozens of stop words) which are present in approximately all texts. Adding them to index will lead to growing index size and practically zero profit.\n",
    "\n",
    "**Implement a function which removes all stop words.**\n",
    "\n",
    "- `tokens` - list of lowercase words.\n",
    "- `stopwords` - `set` of words to exclude.\n",
    "- `returns` - `list` of words without stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_tokens(tokens, stopwords):\n",
    "    result = []\n",
    "    #TODO: write here the code which filters result list\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            result.append(token)\n",
    "    \n",
    "    assert not any([token in stopwords for token in result]), \"No stopwords in resulting list.\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure your filtering works ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who let the dogs out? => ['let', 'dogs']\n",
      "Let's go, guys! => [\"let's\", 'go', 'guys']\n",
      "Mamma mia!!! => ['mamma', 'mia']\n",
      "Here is the first sentence. And here is the second... => ['first', 'sentence', 'second']\n"
     ]
    }
   ],
   "source": [
    "for text in test_texts:\n",
    "    print(text, \"=>\", clear_tokens(sentence_to_words(text), stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Inverted index ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here comes index builing! **Please add necessary code that builds inverted index.**\n",
    "\n",
    "- `texts` - `list`, holding `string`s.\n",
    "- `returns` - `dict` with lowercase token (word) as key and `set` of text indices as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(texts):\n",
    "    iindex = defaultdict(set)\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        for tok in clear_tokens(sentence_to_words(text), stopwords):\n",
    "            #TODO: write here the code which prepares inverted index\n",
    "            if tok in text:\n",
    "                iindex[tok].add(i)\n",
    "    \n",
    "                \n",
    "    assert iindex, \"Empty index\"\n",
    "    assert \"disney\" in iindex, \"Word is missing\"\n",
    "    assert iindex[\"disney\"], \"Something wrong with posting list\"\n",
    "    \n",
    "    return iindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build inverted index for our real data and measure time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index build took 4756.64 ms\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "inverted_index = build_inverted_index(texts)\n",
    "finish = time.time()\n",
    "print(\"Inverted index build took {:.2f} ms\".format(1000 * (finish - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add here necessary code to perform search in inverted index.**\n",
    "\n",
    "- `query` - text we search for.\n",
    "- `texts` - `list` of original texts.\n",
    "- `ii` - `dict` holding inverted index.\n",
    "- `returns` - `list` of strings from texts, subset of texts relevant for query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_inv_index(query, texts, ii):\n",
    "    result = []\n",
    "    q = clear_tokens(sentence_to_words(query), stopwords)\n",
    "    postings = None\n",
    "    lists = list()\n",
    "    for iq in q:\n",
    "        lists.append(ii[iq])\n",
    "    if len(lists) > 0:    \n",
    "        postings = lists[0]\n",
    "        for l in lists:\n",
    "            postings = postings.intersection(l)\n",
    "        \n",
    "    #TODO: find here intersection of all posting lists, corresponding to words in q\n",
    "    \n",
    "    result = [texts[idx] for idx in postings or [] if query in texts[idx]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test code. Is it faster? Do you have the same output sets? If no, can you explain, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: go home\n",
      "Time: 0.496 ms\n",
      "Size: 28\n",
      "Sample: [\"`you're all clear, kid, now let's blow this thing & go home` is spoken just before this is destroyed in a 1977 movie\", \"win an academy award & you'll go home with this golden guy\"]\n",
      "\n",
      "Query: who said\n",
      "Time: 1.488 ms\n",
      "Size: 54\n",
      "Sample: [\"famous blonde of the '30s who said, `when women go wrong, men go right after them`\", \"it's the last name of the character who said, `every time a bell rings, an angel gets his wings`\"]\n",
      "\n",
      "Query: what was the\n",
      "Time: 0.000 ms\n",
      "Size: 0\n",
      "Sample: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in test_queries:\n",
    "    print(\"Query:\", query)\n",
    "    start = time.time()\n",
    "    ans = search_in_inv_index(query, texts, inverted_index)\n",
    "    finish = time.time()\n",
    "    print(\"Time: {:.3f} ms\".format(1000 * (finish - start)))\n",
    "    print(\"Size:\", len(ans))\n",
    "    print(\"Sample:\", ans[:2])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because we delete all stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding ##\n",
    "Embedding is a beatiful concept, which allows to bring completely different mathematics to search structures. If we can find a good approach for embedding some object (text, picture, voice, person, etc), then we can utilize cool tricks from linear algebra and data structures to speed up search operations.\n",
    "### 2.1. Lexicon ###\n",
    "Fundamental idea of [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics) is that text meaning (semantics) is hidden in words and their close groupings (near context). Of course, this is just a hypothesis, but it was empirically \"proven\" by multiple useful applications.\n",
    "\n",
    "**Please write a code, which extracts lexicon and inverted lexicon from texts.**\n",
    "\n",
    "`texts` - `list` of texts.\n",
    "\n",
    "`returns` \n",
    " - `lexicon` = `list` of words (`i -> word`); \n",
    " - `inv_lexicon` = `dict` of `lexicon`<sup>-1</sup> (`word -> i`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_and_inverted_lexicon(texts):\n",
    "    lexicon = set()\n",
    "    inv_lexicon = defaultdict()\n",
    "        \n",
    "    for text in texts:\n",
    "        toks = clear_tokens(sentence_to_words(text), stopwords)\n",
    "            # TODO: fill the lexicon list with unique words (order not important)\n",
    "            # and build an inverted version\n",
    "        for tok in toks:\n",
    "            lexicon.add(tok)\n",
    "        \n",
    "    for i, tok in enumerate(lexicon):\n",
    "        inv_lexicon[tok] = i\n",
    "         \n",
    "    if texts:\n",
    "        assert len(lexicon) == len(inv_lexicon), \"Container sizes should match\"\n",
    "    return lexicon, inv_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build lexicon just once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon building time: 3803.88 ms\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Lex, iLex = get_lexicon_and_inverted_lexicon(texts)\n",
    "finish = time.time()\n",
    "print(\"Lexicon building time: {:.2f} ms\".format(1000 * (finish - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just encode our texts in lexicon-dimensional space (TF-IDF of just 0-s and 1-s), we will need 200K vectors of 123K of `float`s. Just to hold this particular matrix, we will have to move to hard drive. Let's avoid this in our tutorial :) \n",
    "\n",
    "PS But PCA still works great for dimensionality reduction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To store matrix we need 216930 * 122917 = 26B floats\n"
     ]
    }
   ],
   "source": [
    "print(\"To store matrix we need {} * {} = {}B floats\".format(len(texts), len(Lex), int(len(texts) * len(Lex) // 1e9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Embedding ###\n",
    "Yes, creating 123K-dimensional vector for each text was very bad idea. Let's use one of smart techniques. I will bring some false positives (some non-related texts will become \"similar\"), but still keeps the idea. This method allows easily tune embedding size to balance between quality and memory.\n",
    "\n",
    "- `text` - text to embed.\n",
    "- `inv_lexicon` - inverted lexicon `dict`.\n",
    "- `dim` - resulting vector size.\n",
    "- `returns` - `dim`-dimensional normed vector (1-dimensional `np.ndarray`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(text, inv_lexicon, dim=100):\n",
    "    result = np.zeros((dim,))\n",
    "    for tok in clear_tokens(sentence_to_words(text), stopwords):\n",
    "        if tok in inv_lexicon:\n",
    "            result[inv_lexicon[tok] % dim] = 1\n",
    "    norm = np.linalg.norm(result)\n",
    "    if norm > 0.0000001:  # yes, I know, magic number :(\n",
    "        result /= norm\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a matrix `M` with all embedding aligned in rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix building time: 6733.20 ms\n"
     ]
    }
   ],
   "source": [
    "D = 100\n",
    "\n",
    "start = time.time()\n",
    "M = np.zeros((len(texts), D))\n",
    "for i, text in enumerate(texts):\n",
    "    M[i, :] = embed(text, iLex)\n",
    "    \n",
    "finish = time.time()\n",
    "print(\"Matrix building time: {:.2f} ms\".format(1000 * (finish - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.31622777],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.30151134, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Search (ANNS - approximate nearest neighbour search) ###\n",
    "As soon as embedding results with a normed vector, cosine similarity equals dot product. Thus, we can multiply `M` by an embedding of query to get a vector of similarities. Here we can use a technique discussed in lecture: pre-ranking set. We take only `top` most similar texts, and check only them for exact match. \n",
    "\n",
    "**Complete the code that performs search.**\n",
    "\n",
    "- `query` - search query.\n",
    "- `texts` - `list` of all texts.\n",
    "- `M` - `|TEXTS|*D` text embedding matrix.\n",
    "- `iLex` - inverted lexicon `dict`.\n",
    "- `top` - size of pre-ranking set.\n",
    "- `returns` - `list` of relevant texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_dot_product(query, texts, M, iLex, top=5000):\n",
    "    result = []\n",
    "    vec = embed(query, iLex)\n",
    "    texts = np.array(texts)\n",
    "    result = texts[np.argsort(M @ vec.transpose())[::-1][:top]]\n",
    "    result = [t for t in result if query in t]\n",
    "    \n",
    "    assert all([query in t for t in result]), \"Results should match query!\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the result of search change? Why? Can we influence this result without rewriting search code? Without rebuilding index matrix `M`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: go home\n",
      "Time: 624.461 ms\n",
      "Size: 28\n",
      "Sample: [\"`you can't go home again` until you name this author who wrote it\", \"the summers are nice in this capital of manitoba--that's when i should have come. me wanna go home\"]\n",
      "\n",
      "Query: who said\n",
      "Time: 556.511 ms\n",
      "Size: 6\n",
      "Sample: ['film character who said, `you should be kissed, and often, and by someone who knows how`', \"character who said, `i'm not bad.  i'm just drawn that way`\"]\n",
      "\n",
      "Query: what was the\n",
      "Time: 524.768 ms\n",
      "Size: 0\n",
      "Sample: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in test_queries:\n",
    "    print(\"Query:\", query)\n",
    "    start = time.time()\n",
    "    ans = search_by_dot_product(query,texts, M, iLex)\n",
    "    finish = time.time()\n",
    "    print(\"Time: {:.3f} ms\".format(1000 * (finish - start)))\n",
    "    print(\"Size:\", len(ans))\n",
    "    print(\"Sample:\", ans[:2])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Small world network ##\n",
    "We discussed [small world networks](https://en.wikipedia.org/wiki/Small-world_network) in lecture. This beautiful concepts utilize skip-list idea to reach query neighbourhood fastly from any random graph node. You have practically all code written, you just need to call `rewire()` function with respect to [Watts–Strogatz process](https://en.wikipedia.org/wiki/Watts%E2%80%93Strogatz_model).\n",
    "\n",
    "**Please write rewiring code.**\n",
    "\n",
    "Function `build_graph` accepts some iterable collection of `values`. In our case this will be embeddings. \n",
    "\n",
    "- `K` is a parameter of Watts–Strogatz model, expressing average degree of graph nodes.\n",
    "- `p` stands for probability of \"rewiring\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    ''' Graph node class. Major properties are `value` to access embedding and `neighbourhood` for adjacent nodes '''\n",
    "    def __init__(self, value, idx):\n",
    "        self.value = value\n",
    "        self.idx = idx\n",
    "        self.neighbourhood = set()\n",
    "        \n",
    "\n",
    "def build_graph(values, K, p=0.4):\n",
    "    '''Accepts container with values. Returns list with graph nodes'''\n",
    "    \n",
    "    def rewire(nodes, i, j, k):\n",
    "        ''' removes i-j connection and adds i-k connection, bi-directional'''\n",
    "        \n",
    "        # trivial case and loop\n",
    "        if j == k or i == k: return False\n",
    "        # parallel edges\n",
    "        if k in nodes[i].neighbourhood: return False\n",
    "        \n",
    "        nodes[i].neighbourhood.remove(j)\n",
    "        nodes[j].neighbourhood.remove(i)\n",
    "        nodes[k].neighbourhood.add(i)\n",
    "        nodes[i].neighbourhood.add(k)\n",
    "        return True\n",
    "    \n",
    "        \n",
    "    N = len(values)\n",
    "    nodes = [None] * N\n",
    "    \n",
    "    # create nodes\n",
    "    for i, val in enumerate(values):\n",
    "        nodes[i] = Node(val, i)\n",
    "    \n",
    "    # create K-regular lattice\n",
    "    for i, val in enumerate(nodes):\n",
    "        for j in range(i - K // 2, i + K // 2 + 1):\n",
    "            if i != j:\n",
    "                nodes[i].neighbourhood.add(j % N)\n",
    "                nodes[j % N].neighbourhood.add(i)\n",
    "    \n",
    "    for i, node in enumerate(nodes):\n",
    "        for j in range(i - K // 2, i):\n",
    "            # TODO: for each node i rewire right hand side i-j edge to some other random node k with probability p\n",
    "            # See Watts–Strogatz model for details\n",
    "            \n",
    "            k = np.random.randint(0, len(nodes)-1)\n",
    "            rewire(nodes, i, j % N, k) \n",
    "\n",
    "#             if i != j:\n",
    "#                 k = np.random.randint(i,len(nodes))\n",
    "#             if i != k:\n",
    "#                 nodes[i].neighbourhood.add(k % N)\n",
    "#                 nodes[k % N].neighbourhood.add(i)\n",
    "            pass\n",
    "                \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigger `K` and `p` you choose, the longer method runs. Bigger `K` leads to bigger near-cliques in a graph and, as a consequence, bigger context to consider at each step of search. Bigger `q` is for a better \"remove hops\", but it should not be close to 1, as it will make graph random (not SW). If you use next block to test previous block, **PLEASE** use small numbers (e.g. `K=10`, `p=0.2`). For final search index use bigger `K` (`K=500`, `p=0.3` works for 90-300 sec. on my machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built in 21240.21 ms\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "G = build_graph(M[:19000,:], K=500, p=0.3)\n",
    "finish = time.time()\n",
    "print(\"Graph built in {:.2f} ms\".format(1000 * (finish - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last step left. We need to implement efficient search procedure which utilize small world properties. You should move current node towards query embedding, keeping fresh nearest neightbours collection. **Please implement basic NSW search**. You can refer to an algorithm at the bottom of page 3 in this [original paper](https://publications.hse.ru/mirror/pubs/share/folder/x5p6h7thif/direct/128296059). Note, that pre-ranking set size will most probably not influence search speed.\n",
    "\n",
    "`search_nsw_basic()`\n",
    "- `query` - `vector` (`np.ndarray`) representing a query.\n",
    "- `nsw` - SW graph.\n",
    "- `top` - re-ranking set size.\n",
    "- `guard_hops` - if method does not converge, we will terminate it.\n",
    "- `returns` - a pair of a `set` of indices and number of hops `(nearest_neighbours_set, hops)`\n",
    "\n",
    "`search_nsw()`\n",
    "- `query` - text query.\n",
    "- `texts` - `list` of texts.\n",
    "- `nsw` - NSW graph.\n",
    "- `iLex` - inverted lexicon.\n",
    "- `returns` - a pair of a `list` of relevant texts and number of hops `(relevant_texts, hops)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nsw_basic(query, nsw, top=5000, guard_hops=64):\n",
    "    ''' basic algorithm, takes vector query and returns a pair (nearest_neighbours, hops)'''\n",
    "    \n",
    "    # I used this in my solution. \n",
    "    # They can do .peekitem() and popitem() to access biggest key\n",
    "    nn = sortedcontainers.SortedDict()\n",
    "    to_visit = sortedcontainers.SortedDict()\n",
    "    visited = set()\n",
    "    current = random.randint(0, len(nsw) - 1)\n",
    "    hops = 0\n",
    "    \n",
    "    # we will break the loop when converged, but we also guard for the case\n",
    "    while hops < guard_hops:\n",
    "        hops += 1\n",
    "        visited.add(current)\n",
    "        sim = np.dot(query, nsw[current].value)\n",
    "        \n",
    "        # TODO: write here you cool code!!!\n",
    "        # I will store here updated sorted list of closes nodes\n",
    "    \n",
    "    # next 4 lines are the part of my solution. You can remove or reuse them\n",
    "    result = set()\n",
    "    while nn and top > 0:\n",
    "        top -= 1\n",
    "        result.add(nn.popitem()[1])\n",
    "\n",
    "    return result, hops\n",
    "\n",
    "        \n",
    "def search_nsw(query, texts, nsw, iLex):\n",
    "    query_embedding = embed(query, iLex)\n",
    "    nn_indices, hops = search_nsw_basic(query_embedding, nsw)\n",
    "    result = [texts[j] for j in nn_indices if query in texts[j]]\n",
    "    return result, hops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most probably your solution will run for some hundreds of milliseconds. But please don't become upset! First of all, this is because this is python. Secondly, you could choose non-optimal parameters or non-optimal data structures for function internal containers (so did I). Third, we used awful embedding function :). And the last but not the least: `O(K×D×log(N))`. Thus, for 200M records your solution will still fit in a second!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in test_queries:\n",
    "    print(\"Query:\", query)\n",
    "    start = time.time()\n",
    "    ans, hops = search_nsw(query, texts, G, iLex)\n",
    "    print(\"Hops:\", hops)\n",
    "    finish = time.time()\n",
    "    print(\"Time: {:.3f} ms\".format(1000 * (finish - start)))\n",
    "    print(\"Size:\", len(ans))\n",
    "    print(\"Sample:\", ans[:2])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
